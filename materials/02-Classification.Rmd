---
title: "02-Classification"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(tune)

fit_split <- function(formula, model, split, ...) {
  wf <- workflows::add_model(workflows::add_formula(workflows::workflow(), formula, blueprint = hardhat::default_formula_blueprint(indicators = FALSE)), model)
  tune::last_fit(wf, split, ...)
}
```

```{r eval = FALSE, include = FALSE}
# get the data and downsample (shh)
library(modeldata)

data(stackoverflow)

set.seed(1)
fake_so <- stackoverflow %>% 
  janitor::clean_names() %>% 
  group_by(remote) %>% 
  sample_n(size = 575)

write_rds(fake_so , 
          path = here::here("Day-1/exercises/data/stackoverflow.rds"))
```


```{r}
# read in the data
stackoverflow <- read_rds(here::here("Day-1/exercises/data/stackoverflow.rds"))

# split the data
set.seed(100) # Important!
so_split <- initial_split(stackoverflow, strata = remote)
so_train <- training(so_split)
so_test  <- testing(so_split)
```

# Day One - How to get good predictions from models  

## Regression vs. Classification (i.e. "Classification")


1 person per team thinks of the animal- 3-4 people

keep track of how many questions;

5 minutes

5 minutes to meet

# Your Turn 1

Using the so_train and so_test datasets, how many individuals in our training set are remote? How about in the testing set?

```{r}
so_train %>% 
  count(remote)

so_test %>% 
  count(remote)
```

# Your Turn 2

Fill in the blanks. Use `fit_split()` and `collect_predictions()` to

1. Fit a classification tree model with the formula `remote ~ years_coded_job + salary`
2. Look at the predictions you've collected- which variable corresponds to the predictions? What kind of variable is it?
3. Keep `set.seed(100)` at the start of your code.  

*Hint: Be sure to remove every `_` before running the code!*

```{r}
dt_spec <- 
  decision_tree() %>%          
  set_engine(engine = "rpart") %>% 
  set_mode("classification")
dt_spec

set.seed(100) # Important!
dt_fit <- fit_split(remote ~ years_coded_job + salary, 
                    model = dt_spec, 
                    split = so_split) 

dt_fit %>%   
  collect_predictions()
```

# Your Turn 3

Add a pipe after the `collect_predictions()` function to count the number of individuals (i.e., rows) by their true and predicted remote status. In groups, answer the following questions:

1. How many predictions did we make?
2. How many times is "remote" status predicted?
3. How many respondents are actually remote?
4. How many predictions did we get right?

```{r}
dt_fit %>%   
  collect_predictions() %>% 
  count(.pred_class, remote)

# show in slides
dt_fit %>%   
  collect_predictions() %>% 
  conf_mat(truth = remote, estimate = .pred_class)

# show in slides
dt_fit %>%   
  collect_metrics()
```

# Your Turn 4

Fill in the blanks. Use `fit_split()`, `metric_set()`, and `collect_metrics()` to estimate:

1. accuracy, 
2. sensitivity, and 
3. specificity. 

What do you notice?

```{r}
set.seed(100) # Important!
dt_fit <- fit_split(remote ~ years_coded_job + salary, 
                    model = dt_spec, 
                    split = so_split,
                    metrics = metric_set(accuracy, sens, spec)) 

dt_fit %>%   
  collect_metrics()
```


# Your Turn 5

Use `metric_set()` to use roc_auc as the performance metric. Answer the following?

1. What is the value- thumbs up or thumbs down?
2. Look at the predictions you've collected- do you notice anything different?
3. Which variable contains the probability that someone is classified as remote?

```{r}
set.seed(100) # Important!
dt_fit <- fit_split(remote ~ years_coded_job + salary, 
                    model = dt_spec, 
                    split = so_split,
                    metrics = metric_set(roc_auc)) 

# what is it?
dt_fit %>%   
  collect_metrics()

# different from before? Yes- probabilities not classes!
dt_fit %>%   
  collect_predictions()
```

# Your Turn 6

Edit the code below to:

1. Add a pipe after `collect_predictions()` to calculate the data needed to construct the full ROC curve- remember you need to name the column with the true class, and the column with the class probabilities.

2. Examine the ROC curve tibble- if you increased/decreased the threshold from 0.5, what would happen to sensitivity? Specificity?

3. Plot the ROC curve- how does it look to you?

```{r}
dt_roc <- dt_fit %>%   
  collect_predictions() %>% 
  roc_curve(remote, .pred_Remote) 

dt_roc

dt_roc %>% 
  autoplot()
```



<!--transition to more on how trees work: splits, early stopping, pruning -->

# Your Turn 7

Pick the splits! I provide the choices, they pick the winning split and derive a set of rules.

# Your Turn 8

overfit trees- discuss arguments min_n and cost_complexity

### Recap: Modeling problems come in two types

With some ingenuity, each of these models can be used for regression and classification problems. A regression problem predicts a number from a number line, a classification problem predicts a class or category. How you evaluate a model will depend on the type of problem:

*  **For regression , use RMSE.** Regression problems predict a number from a number line. You can use something like the RMSE to measure how much your modelâ€™s predictions miss the truth on average. 

*  **For classification problems, use truth tables.** You can summarize a truth table with specificity, sensitivity, or ROC AUC, which is a useful way of reducing a truth table to a single number.

In each case, better predictions  = better models.

